#Coleta de NotÃ­cias (Web Scraper)

![Status](https://img.shields.io/badge/status-funcional-green)
![Python](https://img.shields.io/badge/Python-3.9+-blue)

## VisÃ£o Geral do Projeto

Este projeto Ã© uma ferramenta de automaÃ§Ã£o construÃ­da em Python. O seu principal objetivo Ã© eliminar o trabalho manual e repetitivo de coletar informaÃ§Ãµes de sites. Ele foi programado para funcionar como um robÃ´ que visita o site, identifica os dados principais e as organiza de forma automÃ¡tica numa planilha Excel, pronta para ser utilizada.

---

## Objetivo do projeto

O processo Ã© dividido em duas missÃµes claras, executadas em sequÃªncia.

## ğŸ› ï¸ Tecnologias Utilizadas

-   **Python 3.9+**
-   **Requests:** Para fazer as requisiÃ§Ãµes HTTP e buscar o conteÃºdo HTML da pÃ¡gina.
-   **BeautifulSoup4 & lxml:** Para analisar (fazer o "parse") do HTML e encontrar os dados de interesse.
-   **Pandas & openpyxl:** Para manipular os dados e criar o arquivo final em formato Excel.


## âš™ï¸ Como Funciona: O Fluxo de Trabalho

O processo Ã© orquestrado pelo arquivo `main.py` e dividido em duas missÃµes claras:

1.  **MissÃ£o 1: A Coleta (Scraping)**
    -   O sistema visita o site.
    -   Ele lÃª o cÃ³digo-fonte HTML da pÃ¡gina.
    -   Dentro desse cÃ³digo, ele procura por padrÃµes que identificam os tÃ­tulos do site e os seus links.
    -   Toda a informaÃ§Ã£o bruta que ele encontra Ã© guardada no arquivo `data/raw_txt/dados_site.txt`.

2.  **MissÃ£o 2: A OrganizaÃ§Ã£o (Processamento)**
    -   O sistema lÃª o arquivo de texto bruto (`dados_site.txt`).
    -   Ele limpa e separa os tÃ­tulos dos links.
    -   Utilizando a biblioteca Pandas, ele monta uma tabela organizada.
    -   Finalmente, esta tabela Ã© exportada como uma planilha Excel (`data/processed_excel/dados_site.xlsx`).

---

# ğŸ“‚ Estrutura de Pastas do Projeto

````
/projeto_final_scraping/
|
â”œâ”€â”€ scrapers/               # ContÃ©m o o sistema que irÃ¡ fazer a coleta
â”‚   â””â”€â”€ site_scraper.py
|
â”œâ”€â”€ processor/              # ContÃ©m os mÃ³dulos de processamento
â”‚   â””â”€â”€ converter.py
|
â”œâ”€â”€ data/                   # Onde os resultados sÃ£o guardados
â”‚   â”œâ”€â”€ raw_txt/            # ContÃ©m os dados da coleta
â”‚   â”‚   â””â”€â”€ dados_site.txt
â”‚   â””â”€â”€ processed_excel/    # ContÃ©m as planilhas finais processadas
â”‚       â””â”€â”€ dados_site.xlsx
|
â”œâ”€â”€ venv/                   # Pasta do ambiente virtual
|
â”œâ”€â”€ main.py                 # Orquestrador principal do projeto
â”œâ”€â”€ requirements.txt        # Lista de bibliotecas necessÃ¡rias
â””â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto

````
---

## Guia de funcionamento 

### Passo 1: Instale as Bibliotecas

Primeiro instale as bibliotecas no seu ambiente python.
```bash
    pip install requests beautifulsoup4 pandas
```

### Passo 2: Crie e Ative o Ambiente virtual

Isto cria um espaÃ§o de trabalho isolado para o projeto, para que as ferramentas (bibliotecas) que instalarmos nÃ£o se misturem com as de outros projetos.

```bash
# 1. Crie o ambiente 
python3 -m venv venv

# 2. Ative o ambiente (precisa de fazer isto sempre que for trabalhar no projeto)

#No macOS ou Linux:
source venv/bin/activate

# No Windows:
venv\Scripts\activate
```

Dica: Quando o ambiente estÃ¡ ativo, o nome (venv) aparecerÃ¡ no inÃ­cio da linha do seu terminal. Isto confirma que estÃ¡ no espaÃ§o de trabalho correto.

### Passo 3: Instale as Ferramentas NecessÃ¡rias

Agora, vamos instalar todas as ferramentas que o projeto precisa para funcionar. O pip faz isso automaticamente lendo a lista de compras (`requirements.txt`).

```bash
# Com o ambiente (venv) ativo, execute:
    pip install -r requirements.txt
```

### Passo 4: Executar o programa
Com tudo pronto, inicie o processo com o seguinte comando:
```bash
# Com o ambiente (venv) ainda ativo, execute:
python main.py
```

O script irÃ¡ rodar e, ao final, a sua planilha Excel estarÃ¡ pronta na pasta data/processed_excel.
