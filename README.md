#Coleta de NotÃ­cias (Web Scraper)

![Status](https://img.shields.io/badge/status-funcional-green)
![Python](https://img.shields.io/badge/Python-3.9+-blue)

## VisÃ£o Geral do Projeto

Este projeto Ã© uma ferramenta de automaÃ§Ã£o construÃ­da em Python. O seu principal objetivo Ã© eliminar o trabalho manual e repetitivo de coletar informaÃ§Ãµes de sites. Ele foi programado para funcionar como um robÃ´ que visita o site, identifica os dados principais e as organiza de forma automÃ¡tica numa planilha Excel, pronta para ser utilizada.

---

## Objetivo do projeto

O processo Ã© dividido em duas missÃµes claras, executadas em sequÃªncia.

## ğŸ› ï¸ Tecnologias Utilizadas

-   **Python 3.9+**
-   **Requests:** Para fazer as requisiÃ§Ãµes HTTP e buscar o conteÃºdo HTML da pÃ¡gina.
-   **BeautifulSoup4 & lxml:** Para analisar (fazer o "parse") do HTML e encontrar os dados de interesse.
-   **Pandas & openpyxl:** Para manipular os dados e criar o arquivo final em formato Excel.


## âš™ï¸ Como Funciona: O Fluxo de Trabalho

O processo Ã© orquestrado pelo arquivo `main.py` e dividido em duas missÃµes claras:

1.  **MissÃ£o 1: A Coleta (Scraping)**
    -   O sistema visita o site.
    -   Ele lÃª o cÃ³digo-fonte HTML da pÃ¡gina.
    -   Dentro desse cÃ³digo, ele procura por padrÃµes que identificam os tÃ­tulos do site e os seus links.
    -   Toda a informaÃ§Ã£o bruta que ele encontra Ã© guardada no arquivo `data/raw_txt/dados_site.txt`.

2.  **MissÃ£o 2: A OrganizaÃ§Ã£o (Processamento)**
    -   O sistema lÃª o arquivo de texto bruto (`dados_site.txt`).
    -   Ele limpa e separa os tÃ­tulos dos links.
    -   Utilizando a biblioteca Pandas, ele monta uma tabela organizada.
    -   Finalmente, esta tabela Ã© exportada como uma planilha Excel (`data/processed_excel/dados_site.xlsx`).

## ğŸ“‚ Estrutura de Pastas do Projeto

````
/Web-Scraping/
|
â”œâ”€â”€ scrapers/               # ContÃ©m o o sistema que irÃ¡ fazer a coleta
â”‚   â””â”€â”€ site_scraper.py
|
â”œâ”€â”€ processor/              # ContÃ©m os mÃ³dulos de processamento
â”‚   â””â”€â”€ converter.py
|
â”œâ”€â”€ data/                   # Onde os resultados sÃ£o guardados
â”‚   â”œâ”€â”€ raw_txt/            # ContÃ©m os dados da coleta
â”‚   â”‚   â””â”€â”€ dados_site.txt
|   |
â”‚   â””â”€â”€ processed_excel/    # ContÃ©m as planilhas finais processadas
â”‚       â””â”€â”€ dados_site.xlsx
|
â”œâ”€â”€ venv/                   # Pasta do ambiente virtual
|
â”œâ”€â”€ main.py                 # Orquestrador principal do projeto
â”œâ”€â”€ requirements.txt        # Lista de bibliotecas necessÃ¡rias
â””â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto

````

## ğŸš€  Guia de funcionamento 

### Passo 1: Instale as Bibliotecas

Primeiro instale as bibliotecas no seu ambiente python.
```bash
    pip install requests beautifulsoup4 pandas
```

### Passo 2: Crie e Ative o Ambiente virtual

Isto cria um espaÃ§o de trabalho isolado para o projeto, para que as ferramentas (bibliotecas) que instalarmos nÃ£o se misturem com as de outros projetos.

```bash
# 1. Crie o ambiente 
python3 -m venv venv

# 2. Ative o ambiente (precisa de fazer isto sempre que for trabalhar no projeto)

#No macOS ou Linux:
source venv/bin/activate

# No Windows:
venv\Scripts\activate
```

Dica: Quando o ambiente estÃ¡ ativo, o nome (venv) aparecerÃ¡ no inÃ­cio da linha do seu terminal. Isto confirma que estÃ¡ no espaÃ§o de trabalho correto.

### Passo 3: Instale as Ferramentas NecessÃ¡rias

Agora, vamos instalar todas as ferramentas que o projeto precisa para funcionar. O pip faz isso automaticamente lendo a lista de compras (`requirements.txt`).

```bash
# Com o ambiente (venv) ativo, execute:
    pip install -r requirements.txt
```

### Passo 4: Executar o programa
Com tudo pronto, inicie o processo com o seguinte comando:
```bash
# Com o ambiente (venv) ainda ativo, execute:
python main.py
```
O script irÃ¡ rodar e, ao final, a sua planilha Excel estarÃ¡ pronta na pasta data/processed_excel.

---
## ConclusÃ£o e PrÃ³ximos Passos

Este projeto serve para mostras como web scraping e automaÃ§Ã£o de dados funciona. Ele demonstra de forma prÃ¡tica como transformar informaÃ§Ãµes dispersas na web em um formato estruturado e Ãºtil, economizando tempo e permitindo anÃ¡lises mais aprofundadas.

Com esta base, as possibilidades de expansÃ£o sÃ£o muitas:

- **Expandir para Outros Sites**: Adaptar para coletar dados de outros sites como fontes de notÃ­cias, e-commerces ou qualquer site de seu interesse.

- **Automatizar a ExecuÃ§Ã£o**: Utilize ferramentas como cron (em macOS/Linux) ou o Agendador de Tarefas (no Windows) para que o script rode automaticamente todos os dias.

- **VisualizaÃ§Ã£o de Dados**: Crie um segundo script que leia a planilha Excel e gere grÃ¡ficos e visualizaÃ§Ãµes sobre as notÃ­cias coletadas.

- **NotificaÃ§Ãµes**: Integre o projeto com um serviÃ§o de e-mail ou mensageiros (como Telegram) para receber as manchetes do dia assim que forem coletadas.

Sinta-se Ã  vontade para experimentar e evoluir este projeto. A automaÃ§Ã£o Ã© uma habilidade poderosa e este Ã© apenas o comeÃ§o!

---
#âš ï¸ Aviso Importante
Projetos de web scraping dependem da estrutura do site-alvo. Se os desenvolvedores do G1 alterarem o layout da pÃ¡gina, o robÃ´ pode deixar de encontrar as informaÃ§Ãµes. Caso isso aconteÃ§a, serÃ¡ necessÃ¡rio inspecionar o novo cÃ³digo do site e atualizar os seletores no arquivo scrapers/g1_scraper.py.

---

